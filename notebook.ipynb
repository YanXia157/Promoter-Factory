{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from Bio import SeqIO\n",
    "\n",
    "genome_file = \"example_genome/Escherichia_coli_K-12_MG1655.fasta\"\n",
    "\n",
    "## 1. predict orfs using prokka\n",
    "\n",
    "def run_prokka(genome_file):\n",
    "    os.makedirs(\"prokka_output\", exist_ok=True)\n",
    "    os.system(f\"cp {genome_file} prokka_output/genome.fna\")\n",
    "    os.chdir(\"prokka_output\")\n",
    "    cmd = f\"prokka --kingdom Bacteria genome.fna --noanno\" # change Bacteria to Archaea if needed\n",
    "    subprocess.run(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "run_prokka(genome_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data4/miniforge3/envs/utr/lib/python3.10/site-packages/Bio/SeqFeature.py:230: BiopythonDeprecationWarning: Please use .location.strand rather than .strand\n",
      "  warnings.warn(\n",
      "100%|██████████| 4300/4300 [01:11<00:00, 60.00it/s]\n"
     ]
    }
   ],
   "source": [
    "## 2. extract upper stream of orfs\n",
    "from glob import glob\n",
    "\n",
    "prokka_gbk_file = glob(\"prokka_output/PROKKA_*/*.gbk\")[0]\n",
    "putative_promoters = []\n",
    "with open(prokka_gbk_file) as input_handle:\n",
    "    for record in SeqIO.parse(input_handle, \"genbank\"):\n",
    "        seq = record.seq\n",
    "        for feature in record.features:\n",
    "            type = feature.type\n",
    "            upper_seq_160bp = None\n",
    "            if type == \"CDS\":\n",
    "                if feature.strand == 1 and feature.location.start > 160:\n",
    "                    upper_seq_160bp = record.seq[feature.location.start-160:feature.location.start]\n",
    "                if feature.location.strand == -1 and feature.location.end < len(record.seq)-160:\n",
    "                    upper_seq_160bp = record.seq[feature.location.end:feature.location.end+160]\n",
    "                    upper_seq_160bp = upper_seq_160bp.reverse_complement()\n",
    "                if upper_seq_160bp:\n",
    "                    putative_promoters.append(upper_seq_160bp)\n",
    "with open(\"prokka_output/putative_promoters.fasta\", \"w\") as output_handle:\n",
    "    for i, sequence in enumerate(putative_promoters):\n",
    "        output_handle.write(f\">seq_{i}\\n{sequence}\\n\")\n",
    "\n",
    "## 3. predict promoters using promotercalculator\n",
    "\n",
    "os.makedirs(\"promotercalculator_output\", exist_ok=True)\n",
    "from Promoter_Calculator_v1_0 import Promoter_Calculator\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "core_promoters = []\n",
    "predicted_Tx_rates = []\n",
    "\n",
    "calc = Promoter_Calculator()\n",
    "\n",
    "def process_sequence(sequence):\n",
    "    calc.run(sequence, TSS_range=[60, len(sequence)])\n",
    "    output = calc.output()\n",
    "    score_list = [result['Tx_rate'] for result in output['Forward_Predictions_per_TSS'].values()]\n",
    "    # print(sequence, score_list)\n",
    "    index = score_list.index(max(score_list))\n",
    "    core_promoter = sequence[index:index + 60]\n",
    "    return sequence, core_promoter, max(score_list)\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(process_sequence)(sequence) for sequence in tqdm(putative_promoters))\n",
    "\n",
    "for result in results:\n",
    "    core_promoters.append(result[1])\n",
    "    predicted_Tx_rates.append(result[2])\n",
    "\n",
    "df = pd.DataFrame({'Putative_Promoter': [result[0] for result in results], 'Core_Promoter': core_promoters, 'Predicted_Tx_Rate': predicted_Tx_rates})\n",
    "df.to_csv(\"promotercalculator_output/predicted_promoters.csv\", index=False)\n",
    "\n",
    "with open(\"promotercalculator_output/predicted_promoters_strong.fasta\", \"w\") as output_handle:\n",
    "    for index, row in df.sort_values(\"Predicted_Tx_Rate\",ascending=False).head(len(df)//5).iterrows():\n",
    "        output_handle.write(f\">{index}|ptx={row['Predicted_Tx_Rate']}\\n{row['Core_Promoter']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. train a model using the predicted promoters\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import warnings\n",
    "\n",
    "# Define the arguments\n",
    "base_model_name = \"jinyuan22/promogen2-xsmall\"\n",
    "fasta_data_path = \"promotercalculator_output/predicted_promoters_strong.fasta\"\n",
    "output_dir = \"examples/promogen2_small_usp\"\n",
    "special_token = \"<|usp|>\"\n",
    "per_device_train_batch_size = 32\n",
    "learning_rate = 5e-5\n",
    "warmup_steps = 1000\n",
    "logging_steps = 1000\n",
    "save_steps = 1000\n",
    "eval_steps = 1000\n",
    "max_steps = 10000\n",
    "fp16 = True\n",
    "seed = 42\n",
    "\n",
    "# Set the seed\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(base_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "num_added_toks = tokenizer.add_special_tokens({\"additional_special_tokens\": [special_token]})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def convert_fasta_to_csv(fasta_path, tag=\"<|usp|>\"):\n",
    "    fasta_dict = {}\n",
    "    with open(fasta_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                header = line.strip()\n",
    "                fasta_dict[header] = \"\"\n",
    "            else:\n",
    "                fasta_dict[header] += line.strip()\n",
    "    names = []\n",
    "    seqs = []\n",
    "    tags = []\n",
    "    ids = []\n",
    "    for k, v in fasta_dict.items():\n",
    "        names.append(k)\n",
    "        seqs.append(v)\n",
    "        tags.append(tag)\n",
    "        ids.append(\"|f\")\n",
    "        names.append(k)\n",
    "        seqs.append(v[::-1])\n",
    "        tags.append(tag)\n",
    "        ids.append(\"|r\")\n",
    "    data = {\"ID\": ids, \"Name\": names, \"Seq\": seqs, \"Tag\": tags}\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(fasta_path + \"_traintext.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "usp_df = convert_fasta_to_csv(fasta_data_path, tag=special_token)\n",
    "total_step = min([max_steps, int(len(usp_df) / per_device_train_batch_size) * 10])\n",
    "\n",
    "if total_step < max_steps:\n",
    "    # warnings\n",
    "    warn_message = f\"There are {len(usp_df)} samples in the dataset. The total steps will be set to {total_step}.\"\n",
    "    warnings.warn(warn_message)\n",
    "    warmup_steps = int(total_step * 0.1)\n",
    "    logging_steps = int(total_step * 0.1)\n",
    "    save_steps = int(total_step * 0.1)\n",
    "    eval_steps = int(total_step * 0.1)\n",
    "\n",
    "train_ds = Dataset.from_pandas(usp_df)\n",
    "\n",
    "\n",
    "def preprocess_function(samples):\n",
    "    processed_samples = {\"input_ids\": [], \"attention_mask\": []}\n",
    "    for i, dna_sequence in enumerate((samples[\"Seq\"])):\n",
    "        tag = samples[\"Tag\"][i]\n",
    "        if samples[\"ID\"][i] == \"|f\":\n",
    "            tokenized_input = tokenizer(\n",
    "                f\"<|bos|>{tag}5{dna_sequence}3{tag}<|eos|>\",\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "                max_length=204,\n",
    "            )\n",
    "        if samples[\"ID\"][i] == \"|r\":\n",
    "            tokenized_input = tokenizer(\n",
    "                f\"<|bos|>{tag}3{dna_sequence}5{tag}<|eos|>\",\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "                max_length=204,\n",
    "            )\n",
    "        processed_samples[\"input_ids\"].append(tokenized_input[\"input_ids\"])\n",
    "        processed_samples[\"attention_mask\"].append(tokenized_input[\"attention_mask\"])\n",
    "    return processed_samples\n",
    "\n",
    "tokenized_ds = train_ds.map(preprocess_function, batched=True, num_proc=8)\n",
    "\n",
    "train_testvalid = tokenized_ds.train_test_split(test_size=0.1)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{output_dir}_output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=0.1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    warmup_steps=warmup_steps,\n",
    "    max_steps=total_step,\n",
    "    logging_steps=logging_steps,\n",
    "    eval_steps=eval_steps,\n",
    "    logging_strategy=\"steps\",\n",
    "    save_steps=save_steps,\n",
    "    report_to=\"tensorboard\",\n",
    "    fp16=fp16,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_testvalid[\"train\"],\n",
    "    eval_dataset=train_testvalid[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(trainer.evaluate())\n",
    "\n",
    "trainer.save_model(f\"{output_dir}\")\n",
    "tokenizer.save_pretrained(f\"{output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. predict promoters using the trained model\n",
    "from transformers import pipeline\n",
    "model_path = output_dir\n",
    "if fp16:\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "else:\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, device=device, tokenizer=tokenizer)\n",
    "\n",
    "@torch.no_grad()\n",
    "def score(seq, tag, device=\"cuda:0\"):\n",
    "    if tag == 'none':\n",
    "        inputs = tokenizer(f\"<|bos|>5{seq}3<|eos|>\", return_tensors=\"pt\")\n",
    "    else:\n",
    "        inputs = tokenizer(f\"<|bos|>{tag}5{seq}3{tag}<|eos|>\", return_tensors=\"pt\")\n",
    "    # inputs = tokenizer(seq.upper(), return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    input_ids  = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    pred = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "    return pred['loss'].item()\n",
    "\n",
    "txt = f\"<|bos|>{special_token}5\"\n",
    "tag = special_token\n",
    "\n",
    "num_return_sequences = 128*50\n",
    "batch_size = 256\n",
    "max_new_tokens = 60\n",
    "repetition_penalty = 1.0\n",
    "top_p = 1.0\n",
    "temperature = 1.0\n",
    "do_sample = True\n",
    "\n",
    "all_outputs = []\n",
    "for i in range(0, num_return_sequences, batch_size):\n",
    "    # txt_batch = txt[i:i+args.batch_size]\n",
    "    outputs = pipe(txt, \n",
    "            num_return_sequences=batch_size,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample\n",
    "        )\n",
    "    all_outputs.extend(outputs)\n",
    "\n",
    "model.eval().to(device)\n",
    "seqs = [output[\"generated_text\"].replace(\"<|bos|>\", \"\").replace(\"5\", \"\").replace(\"3\", \"\").replace(tag, \"\") for output in all_outputs]\n",
    "scores = [score(seq, tag) for seq in seqs]\n",
    "\n",
    "sp_name = special_token.replace(\"<|\", \"\").replace(\"|>\", \"\")\n",
    "output_file_name = f\"examples/{sp_name}_t_{temperature}_r_{repetition_penalty}_p_{top_p}.fasta\"\n",
    "\n",
    "with open(output_file_name, \"w\") as f:\n",
    "    for i, (seq, score) in enumerate(zip(seqs, scores)):\n",
    "        f.write(f\">{i}|score={score}\\n{seq}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def fasta_to_list(fasta_file):\n",
    "    \"\"\"Convert a fasta file to a list.\"\"\"\n",
    "    fasta_seqs = []\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                header = line[1:]\n",
    "                fasta_seqs.append(\"\")\n",
    "            else:\n",
    "                fasta_seqs[-1] += line\n",
    "    return fasta_seqs\n",
    "\n",
    "def generate_all_possible_kmers(k):\n",
    "    \"\"\"Generate all possible k-mers of a given length k for DNA using itertools.\"\"\"\n",
    "    bases = ['A', 'T', 'C', 'G']\n",
    "    return [''.join(p) for p in itertools.product(bases, repeat=k)]\n",
    "\n",
    "six_mers = generate_all_possible_kmers(6)\n",
    "\n",
    "def count_kmers_in_sequences(sequences, k=6):\n",
    "    \"\"\"Count occurrences of all k-mers in a list of sequences.\"\"\"\n",
    "    all_kmers = generate_all_possible_kmers(k)\n",
    "    kmers_count = {kmer: 0 for kmer in all_kmers}\n",
    "    \n",
    "    for seq in sequences:\n",
    "        for i in range(len(seq) - k + 1):\n",
    "            kmer = seq[i:i+k]\n",
    "            if kmer in kmers_count:\n",
    "                kmers_count[kmer] += 1\n",
    "\n",
    "    return kmers_count\n",
    "\n",
    "native_fasta_file = \"promotercalculator_output/predicted_promoters_strong.fasta\"\n",
    "native_promoters = fasta_to_list(native_fasta_file)\n",
    "native_promoters_kmer_count = count_kmers_in_sequences(native_promoters)\n",
    "\n",
    "generate_promoter_file = output_file_name\n",
    "generate_promoters = fasta_to_list(generate_promoter_file)\n",
    "generate_promoters = [s for s in generate_promoters if len(s) == 60]\n",
    "generate_promoters_kmer_count = count_kmers_in_sequences(generate_promoters[:len(native_promoters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, pearsonr\n",
    "rho = spearmanr(list(native_promoters_kmer_count.values()), list(generate_promoters_kmer_count.values()))[0]\n",
    "prs = pearsonr(list(native_promoters_kmer_count.values()), list(generate_promoters_kmer_count.values()))[0]\n",
    "print(f\"spearman: {rho:.3f}, pearson: {prs:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import .pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.regplot(x=np.array(list(native_promoters_kmer_count.values()))/np.sum(list(native_promoters_kmer_count.values())), \n",
    "            y=np.array(list(generate_promoters_kmer_count.values()))/np.sum(list(native_promoters_kmer_count.values())),\n",
    "            scatter_kws={'s': 4}, \n",
    "            label=f\"Spearman correlation: {rho:.4f}\\nPearson correlation: {prs:.4f}\")\n",
    "plt.xlabel(\"Native promoters\")\n",
    "plt.ylabel(\"Generated promoters\")\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
